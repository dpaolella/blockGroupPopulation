{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of state FIPS codes\n",
    "fips = ['01', '04', '05', '06', '08', '09', '10', \n",
    "        '11', '12', '13', '16' ,'17', '18', '19', \n",
    "        '20', '21', '22', '23', '24', '25', '26',\n",
    "        '27', '28', '29', '30', '31', '32', '33', \n",
    "        '34', '35', '36', '37', '38', '39', '40', \n",
    "        '41', '42', '44', '45', '46', '47', '48', \n",
    "        '49', '50', '51', '53', '54', '55', '56']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import ACS data (2007-2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nhgis0027_ds195_20095_2009_blck_grp.csv\n",
      "nhgis0027_ds206_20145_2014_blck_grp.csv\n",
      "nhgis0027_ds176_20105_2010_blck_grp.csv\n",
      "nhgis0027_ds225_20165_2016_blck_grp.csv\n",
      "nhgis0027_ds215_20155_2015_blck_grp.csv\n",
      "nhgis0027_ds184_20115_2011_blck_grp.csv\n",
      "nhgis0027_ds201_20135_2013_blck_grp.csv\n"
     ]
    }
   ],
   "source": [
    "raceMap = {'E001': 'all', 'E012': 'latino', 'E003': 'white', 'E004': 'black', \n",
    "           'E005': 'native', 'E006': 'asian', 'E007': 'hwPac'}\n",
    "raceKeys = list(raceMap.keys())\n",
    "\n",
    "popData07_14 = pd.DataFrame()\n",
    "\n",
    "dataPath = 'raw/acs'\n",
    "for filename in os.listdir(dataPath):\n",
    "    if filename.endswith('.csv'):\n",
    "        print(filename)\n",
    "        \n",
    "        nhgisCode=''\n",
    "        with open(os.path.join(dataPath, filename[:-4] + '_codebook.txt'), 'r') as codebook:\n",
    "            cb = codebook.read().replace('\\n', '')\n",
    "            nhgisCode = cb[cb.find('E001') - 4: cb.find('E001')].strip()\n",
    "            \n",
    "        columns = [nhgisCode + s for s in raceKeys]\n",
    "        \n",
    "        df = pd.read_csv(os.path.join(dataPath, filename), encoding = \"ISO-8859-1\")\n",
    "        df = df[['GISJOIN', 'YEAR', 'STATE', 'STATEA', 'COUNTY', 'COUNTYA'] + columns]\n",
    "        df.COUNTYA = df.COUNTYA.astype(str)\n",
    "        df.STATEA = df.STATEA.astype(str)\n",
    "        df.COUNTYA = np.where(df.COUNTYA.str.len() == 1, '00' + df.COUNTYA, df.COUNTYA)\n",
    "        df.COUNTYA = np.where(df.COUNTYA.str.len() == 2, '0' + df.COUNTYA, df.COUNTYA)\n",
    "        df.STATEA = np.where(df.STATEA.str.len() == 1, '0' + df.STATEA, df.STATEA)\n",
    "        df['countyCode'] = df.STATEA + df.COUNTYA\n",
    "        df['year'] = df.YEAR.str.slice(0,4).astype(int) + 2\n",
    "        df['state'] = df.STATE\n",
    "        \n",
    "        df = pd.melt(df, id_vars=['GISJOIN', 'state', 'year'], value_vars=columns)\n",
    "        df.rename(columns={'value': 'pop', 'variable': 'race'}, inplace=True)\n",
    "        df.race = df.race.str.slice(-4)\n",
    "        df.race = df.race.replace(raceMap)\n",
    "        \n",
    "        popData07_14 = popData07_14.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove non-contiguous states/territories\n",
    "popData07_14 = popData07_14.loc[~popData07_14.state.isin(['Puerto Rico', 'Alaska', 'Hawaii'])]\n",
    "\n",
    "popData07_14 = pd.pivot_table(popData07_14, values = 'pop', \n",
    "                              index=['GISJOIN', 'year'], \n",
    "                              columns = 'race').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Decenial Census data (2000/2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2000\n",
    "df = pd.read_csv('raw/census/nhgis0028_ds147_2000_blck_grp.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "raceMap = {'FYF001': 'white', 'FYF002': 'black', 'FYF003': 'native', 'FYF004': 'asian', \n",
    "           'FYF005': 'hwPac', 'FYF006': 'otherNonLat', 'FYF007': 'twoNonLat', \n",
    "           'FYF008': 'lat_white', 'FYF009': 'lat_black', 'FYF010': 'lat_native', \n",
    "           'FYF011': 'lat_asian', 'FYF012': 'lat_hwPac', 'FYF013': 'lat_other', 'FYF014':'lat_two'}\n",
    "raceKeys = list(raceMap.keys())\n",
    "\n",
    "df = df[['GISJOIN', 'YEAR', 'STATE'] + raceKeys]\n",
    "df['latino'] = df.FYF008 + df.FYF009 + df.FYF010 + df.FYF011 + df.FYF012 + df.FYF013 + df.FYF014\n",
    "df['all'] = df.FYF001 + df.FYF002 + df.FYF003 + df.FYF004 + df.FYF005 + df.FYF006 + df.FYF007 + \\\n",
    "            df.FYF008 + df.FYF009 + df.FYF010 + df.FYF011 + df.FYF012 + df.FYF013 + df.FYF014\n",
    "df.drop(['FYF006', 'FYF007','FYF008', 'FYF009', 'FYF010', \n",
    "         'FYF011', 'FYF012', 'FYF013', 'FYF014'], axis=1, inplace=True)\n",
    "df['year'] = df.YEAR\n",
    "df['state'] = df.STATE\n",
    "\n",
    "popData2000 = pd.melt(df, id_vars=['GISJOIN', 'state', 'year'], value_vars=['all', 'latino', 'FYF001', 'FYF002', \n",
    "                                                                   'FYF003', 'FYF004', 'FYF005'])\n",
    "popData2000.rename(columns={'value': 'pop', 'variable': 'race'}, inplace=True)\n",
    "popData2000.race = popData2000.race.replace(raceMap)\n",
    "popData2000 = popData2000.loc[~popData2000.state.isin(['Puerto Rico', 'Alaska', 'Hawaii'])]\n",
    "\n",
    "popData2000 = pd.pivot_table(popData2000, values = 'pop', \n",
    "                              index=['GISJOIN', 'year'], \n",
    "                              columns = 'race').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2010\n",
    "df = pd.read_csv('raw/census/nhgis0028_ds172_2010_blck_grp.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "raceMap = {'H7Z001':'all', 'H7Z010': 'latino', 'H7Z003': 'white', 'H7Z004': 'black', \n",
    "           'H7Z005': 'native', 'H7Z006': 'asian', 'H7Z007': 'hwPac'}\n",
    "raceKeys = list(raceMap.keys())\n",
    "\n",
    "df = df[['GISJOIN', 'YEAR', 'STATE', 'STATEA', 'COUNTY', 'COUNTYA'] + raceKeys]\n",
    "df['year'] = df.YEAR\n",
    "df['state'] = df.STATE\n",
    "\n",
    "popData2010 = pd.melt(df, id_vars=['GISJOIN', 'state', 'year'], value_vars=raceMap)\n",
    "popData2010.rename(columns={'value': 'pop', 'variable': 'race'}, inplace=True)\n",
    "popData2010.race = popData2010.race.replace(raceMap)\n",
    "popData2010 = popData2010.loc[~popData2010.state.isin(['Puerto Rico', 'Alaska', 'Hawaii'])]\n",
    "\n",
    "popData2010 = pd.pivot_table(popData2010, values = 'pop', \n",
    "                              index=['GISJOIN', 'year'], \n",
    "                              columns = 'race').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "popData = popData2000.append(popData2010)\n",
    "popData = popData.append(popData07_14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "years = list(popData.year.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapefiles/pop_2008.shp\n",
      "shapefiles/pop_2009.shp\n",
      "shapefiles/pop_2011.shp\n",
      "shapefiles/pop_2012.shp\n",
      "shapefiles/pop_2013.shp\n",
      "shapefiles/pop_2014.shp\n"
     ]
    }
   ],
   "source": [
    "for y in years:\n",
    "    popDataSubset = popData.loc[popData.year == y]\n",
    "\n",
    "    if y < 2008:\n",
    "        fp = \"shapefiles/US_blck_grp_2000.shp\"\n",
    "        data = gpd.read_file(fp)\n",
    "        data = data.loc[data.FIPSSTCO.str.slice(0,2).isin(fips)]\n",
    "    elif y == 2008:\n",
    "        fp = \"shapefiles/US_blck_grp_2010.shp\"\n",
    "        data = gpd.read_file(fp)\n",
    "        data = data.loc[data.STATEFP10.isin(fips)]         \n",
    "    elif y == 2010:\n",
    "        fp = \"shapefiles/US_blck_grp_2010.shp\"\n",
    "        data = gpd.read_file(fp)\n",
    "        data = data.loc[data.STATEFP10.isin(fips)] \n",
    "    else:\n",
    "        fp = \"shapefiles/US_blck_grp_\" + str(y+2) + \".shp\"\n",
    "        data = gpd.read_file(fp)\n",
    "        data = data.loc[data.STATEFP.isin(fips)] \n",
    "\n",
    "    data = data[['GISJOIN', 'geometry']]\n",
    "    data = data.merge(popDataSubset, how='outer', on='GISJOIN', indicator=True)\n",
    "    if sum(data.loc[data._merge == 'right_only']['all']) > 300000:\n",
    "        print(\"More than 300,000 people are being dropped in merge\")\n",
    "        break\n",
    "    elif len(data) > 250000:\n",
    "        print(\"Bad merge: too many blocks\")\n",
    "        break\n",
    "    else:\n",
    "        data = data.loc[data._merge != 'right_only']\n",
    "        data.drop(['_merge'], axis=1, inplace=True)\n",
    "        s = {'geometry': 'Polygon', 'properties': {'GISJOIN': 'str:20',\n",
    "                                                   'year': 'int:4', \n",
    "                                                   'all': 'int', \n",
    "                                                   'asian': 'int', \n",
    "                                                   'black': 'int',\n",
    "                                                   'hwPac': 'int',\n",
    "                                                   'latino': 'int', \n",
    "                                                   'native': 'int', \n",
    "                                                   'white': 'int'}}\n",
    "        outfp = r\"shapefiles/pop_\" + str(y) + \".shp\"\n",
    "        data.to_file(outfp, schema=s)\n",
    "        print(outfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
